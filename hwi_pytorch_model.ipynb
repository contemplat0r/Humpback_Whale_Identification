{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import skimage\n",
    "from skimage import transform as skimg_transform\n",
    "#from skimage import io as img_io\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import load_data\n",
    "import model\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from '/home/uldo/work/kaggle/competitions/Humpback_Whale_Identification/code/model.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.style.use('Solarize_Light2')\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "importlib.reload(load_data)\n",
    "importlib.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 40\n",
    "W = 128\n",
    "H = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HWI_ConvNeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HWI_ConvNeuralNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 12, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.out1 = nn.Linear(int(12 * W/4 * H/4), 900)\n",
    "        self.out2 = nn.Linear(900, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.drop_out(x)\n",
    "        output = self.out1(x)\n",
    "        output = self.out2(output)\n",
    "        #return output, x\n",
    "        return output[:, 0]\n",
    "        #return F.log_softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        orig_height, orig_width = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if orig_height > orig_width:\n",
    "                new_height, new_width = self.output_size * orig_height / orig_width, self.output_size\n",
    "            else:\n",
    "                new_height, new_width = self.output_size, self.output_size * orig_width / orig_height\n",
    "        else:\n",
    "            new_height, new_width = self.output_size\n",
    "\n",
    "        new_height, new_width = int(new_height), int(new_width)\n",
    "\n",
    "        img = skimg_transform.resize(image, (new_height, new_width))\n",
    "\n",
    "        return {'image': img, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifyRescale(object):\n",
    "    \n",
    "    def __init__(self, output_size=128):\n",
    "        assert isinstance(output_size, int)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        img = skimg_transform.resize(image, (self.output_size, self.output_size))\n",
    "\n",
    "        return {'image': img, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        orig_height, orig_width = image.shape[:2]\n",
    "        new_height, new_width = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, orig_height - new_height)\n",
    "        left = np.random.randint(0, orig_width - new_width)\n",
    "\n",
    "        image = image[top: top + new_height, left: left + new_width]\n",
    "\n",
    "        return {'image': image, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __init__(self, image_size=128):\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        \n",
    "        \"\"\" The original code didn't expect gray scale images \"\"\"\n",
    "        \n",
    "        gray_scale_image = torch.zeros(\n",
    "            [self.image_size, self.image_size]\n",
    "        ).shape == image.shape\n",
    "        if gray_scale_image:\n",
    "            image = np.stack((image,) * 3, axis=-1)\n",
    "        \n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image), 'label': torch.tensor(label, dtype=torch.uint8)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_whale_batch(sample_batched):\n",
    "    \"\"\"Show whales for a batch of samples.\"\"\"\n",
    "    images_batch = sample_batched['image']\n",
    "    labels_batch = sample_batched['label']\n",
    "    batch_size = len(images_batch)\n",
    "    im_size = images_batch.size(2)\n",
    "\n",
    "    grid = utils.make_grid(images_batch)\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        plt.title('Batch from dataloader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(submodule):\n",
    "    if type(submodule) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(submodule.weight)\n",
    "        submodule.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders(dataset, valid_train_ratio=0.6):\n",
    "    dataset_size = len(dataset)\n",
    "    print(\"dataset_size: \", dataset_size)\n",
    "\n",
    "    validation_subset_size = int(dataset_size * (1 - valid_train_ratio))\n",
    "    print(\"validation_subset_size: \", validation_subset_size)\n",
    "\n",
    "    indices = list(range(dataset_size))\n",
    "    validation_indices = np.random.choice(indices, size=validation_subset_size, replace=False)\n",
    "    train_indices = list(set(indices) - set(validation_indices))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    validation_sampler = SubsetRandomSampler(validation_indices)\n",
    "    \n",
    "    dataset_sizes = {\n",
    "            'train': len(train_indices),\n",
    "            'validation': len(validation_indices)\n",
    "        }\n",
    "\n",
    "    train_loader = data.DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=1, sampler=train_sampler)\n",
    "    validation_loader = data.DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=1, sampler=validation_sampler)\n",
    "\n",
    "    loaders = {\n",
    "            'train': train_loader,\n",
    "            'validation': validation_loader\n",
    "        }\n",
    "\n",
    "    return loaders, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfold_batch(batch):\n",
    "    return batch['image'], batch['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch_train(model, data_loader, criterion, optimizer):\n",
    "    accuracy = 0.0\n",
    "    total_loss = 0.0\n",
    "    for i, data_batch in enumerate(data_loader, 0):\n",
    "        \n",
    "        inputs, labels = unfold_batch(data_batch)\n",
    "        print(\"inputs: \", inputs)\n",
    "        print(\"labels: \", labels)\n",
    "        inputs = inputs.to(device, dtype=torch.float)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = hwi_conv_neural_net(inputs)\n",
    "\n",
    "        print(\"outputs.size():\\n\", outputs.size())\n",
    "        print(\"labels.size():\\n\", labels.size())\n",
    "        print(\"outputs:\\n\", outputs)\n",
    "        #print(\"outputs[:, 0]:\\n\", outputs[:, 0])\n",
    "        print(\"labels:\\n\", labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        predicted = outputs > 0\n",
    "        print(\"type(predicted): \", type(predicted))\n",
    "        print(\"predicted:\\n\", predicted)\n",
    "\n",
    "        #total += labels.size(0)\n",
    "        labels = labels.data.byte()\n",
    "        print(\"predicted == labels:\\n\", predicted == labels)\n",
    "        #sum_of_correct_predicted = torch.sum((predicted == labels).all(1))\n",
    "        sum_of_correct_predicted = torch.sum((predicted == labels))\n",
    "        item = sum_of_correct_predicted.item()\n",
    "        correct_predicted_total += item\n",
    "    accuracy = correct_predicted_total\n",
    "    \n",
    "    #epoch_train_loss = total_loss / train_dataset_size\n",
    "    #epoch_train_accuracy = correct_predicted_total / train_dataset_size\n",
    "    return (total_loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, validation_loader, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        correct_predicted_total = 0.0\n",
    "        total_loss = 0.0\n",
    "        #total = 0.0\n",
    "        \n",
    "        for data_batch in validation_loader:\n",
    "            inputs, labels = unfold_batch(data_batch)\n",
    "\n",
    "            inputs = inputs.to(device, dtype=torch.float)\n",
    "            #labels_as_float = labels.to(device, dtype=torch.float)\n",
    "            #labels = labels.to(device, dtype=torch.uint8)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            #loss = criterion(outputs, labels_as_float)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            predicted = outputs > 0\n",
    "\n",
    "            #total += labels.size(0)\n",
    "            labels = labels.data.byte()\n",
    "            #sum_of_correct_predicted = torch.sum((predicted == labels).all(1))\n",
    "            sum_of_correct_predicted = torch.sum((predicted == labels))\n",
    "            item = sum_of_correct_predicted.item()\n",
    "\n",
    "            correct_predicted_total += item\n",
    "\n",
    "        #accuracy = correct_predicted_total / total\n",
    "        accuracy = correct_predicted_total\n",
    "        \n",
    "\n",
    "    return (total_loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch_validate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        correct_predicted_total = 0.0\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for data_batch in data_loader:\n",
    "            inputs, labels = unfold_batch(data_batch)\n",
    "\n",
    "            inputs = inputs.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            predicted = outputs > 0\n",
    "            \n",
    "            labels = labels.data.byte()\n",
    "            sum_of_correct_predicted = torch.sum((predicted == labels))\n",
    "            item = sum_of_correct_predicted.item()\n",
    "\n",
    "            correct_predicted_total += item\n",
    "\n",
    "        accuracy = correct_predicted_total        \n",
    "\n",
    "    return (total_loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch_model_process(model, loader, criterion, optimizer=None):\n",
    "    accuracy = 0.0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for data_batch in loader:\n",
    "        inputs, labels = unfold_batch(data_batch)\n",
    "        inputs = inputs.to(device, dtype=torch.float)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "        outputs = model(inputs)[:, 0]\n",
    "            \n",
    "        loss = criterion(outputs, labels)\n",
    "        if optimizer:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "        predicted = outputs > 0\n",
    "\n",
    "        labels = labels.data.byte()\n",
    "        #torch_sum_of_correct_predicted = torch.sum((predicted == labels).all(1))\n",
    "        torch_sum_of_correct_predicted = torch.sum((predicted == labels))\n",
    "        correct_predicted = torch_sum_of_correct_predicted.item()\n",
    "\n",
    "        accuracy += correct_predicted        \n",
    "\n",
    "    return (total_loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_of_epoch, model, dataset_loaders, dataset_sizes, criterion, optimizer):\n",
    "    torch.cuda.empty_cache()\n",
    "    since = time.time()\n",
    "    \n",
    "    train_loader = dataset_loaders['train']\n",
    "    validation_loader = dataset_loaders['validation']\n",
    "    train_dataset_size = dataset_sizes['train']\n",
    "    validation_dataset_size = dataset_sizes['validation']\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    validation_losses = []\n",
    "    validation_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_of_epoch):\n",
    "        \n",
    "        train_loss, train_accuracy = one_epoch_train(model, train_loader, criterion, optimizer)\n",
    "        train_losses.append(train_loss / train_dataset_size)\n",
    "        train_accuracies.append(train_accuracy / train_dataset_size)\n",
    "        \n",
    "        validation_loss, validation_accuracy = one_epoch_validate(model, validation_loader, criterion)\n",
    "        validation_losses.append(validation_loss / validation_dataset_size)\n",
    "        validation_accuracies.append(validation_accuracy / validation_dataset_size)\n",
    "        \n",
    "        print(\"Epoch {}: train loss {}, train accuracy\"\n",
    "          \" {}, validation loss {}, validation accuracy {}\".format(\n",
    "              epoch + 1,\n",
    "              train_loss / train_dataset_size,\n",
    "              train_accuracy / train_dataset_size,\n",
    "              validation_loss / validation_dataset_size,\n",
    "              validation_accuracy / validation_dataset_size\n",
    "            )\n",
    "        )\n",
    "    print(\"Finished Training\")\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\n",
    "            'Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60\n",
    "        )\n",
    "    )\n",
    "    return train_losses, validation_losses, train_accuracies, validation_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, batch):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    #inputs = batch\n",
    "    #inputs = inputs.to(device, dtype=torch.float)\n",
    "    inputs = batch.to(device, dtype=torch.float)\n",
    "    outputs = model(inputs)\n",
    "    #return outputs[0].cpu()\n",
    "    return outputs.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, full=True, name='model'):\n",
    "    if not full:\n",
    "        torch.save(model.state_dict(), '{}_params.pkl'.format(name))\n",
    "    else:\n",
    "        torch.save(model, '{}.pkl'.format(name))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_model(name='model'):\n",
    "    return torch.load('{}.pkl'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data.load_text_data('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_two_classes = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_two_classes.loc[train_df_two_classes['Id'] != 'new_whale', 'Id'] = 'not_new_whale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_two_classes.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 128\n",
    "dataset = load_data.HumpbackWhalesDataset(\n",
    "    train_df_two_classes,\n",
    "    #train_df,\n",
    "    #transform=load_data.transforms.ToTensor()\n",
    "    #transform=ToTensor()\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            #Rescale(int(image_size*1.25)),\n",
    "            Rescale(int(image_size)),\n",
    "            #RandomCrop(image_size),\n",
    "            UnifyRescale(int(image_size)),\n",
    "            ToTensor()\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loaders, dataset_sizes = prepare_loaders(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dataset_loaders['train']\n",
    "validation_loader = dataset_loaders['validation']\n",
    "train_dataset_size = dataset_sizes['train']\n",
    "validation_dataset_size = dataset_sizes['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset_size)\n",
    "print(validation_dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = train_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    print(i_batch, sample_batched['image'].size(),\n",
    "          sample_batched['label'])\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 3:\n",
    "        plt.figure(figsize=(24, 24))\n",
    "        show_whale_batch(sample_batched)\n",
    "        plt.axis('off')\n",
    "        #plt.ioff()\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hwi_conv_neural_net = HWI_ConvNeuralNet()\n",
    "hwi_conv_neural_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hwi_conv_neural_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.BCEWithLogitsLoss(reduction='sum')\n",
    "optimizer = optim.Adam(hwi_conv_neural_net.parameters(), lr=0.001, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_epoch = 12\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "validation_losses = []\n",
    "validation_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "since = time.time()\n",
    "for epoch in range(num_of_epoch):\n",
    "    epoch_train_accuracy = 0.0\n",
    "    epoch_train_loss = 0.0\n",
    "    correct_predicted_total = 0.0\n",
    "    train_loss = 0.0\n",
    "    #total_loss = 0.0\n",
    "    #total = 0.0\n",
    "    for i, data_batch in enumerate(train_loader, 0):\n",
    "        \n",
    "        inputs = data_batch['image']\n",
    "        labels = data_batch['label']\n",
    "        print(\"inputs: \", inputs)\n",
    "        print(\"labels: \", labels)\n",
    "        inputs = inputs.to(device, dtype=torch.float)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = hwi_conv_neural_net(inputs)\n",
    "\n",
    "        print(\"outputs.size():\\n\", outputs.size())\n",
    "        print(\"labels.size():\\n\", labels.size())\n",
    "        print(\"outputs:\\n\", outputs)\n",
    "        #print(\"outputs[:, 0]:\\n\", outputs[:, 0])\n",
    "        print(\"labels:\\n\", labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        predicted = outputs > 0\n",
    "        print(\"type(predicted): \", type(predicted))\n",
    "        print(\"predicted:\\n\", predicted)\n",
    "\n",
    "        #total += labels.size(0)\n",
    "        labels = labels.data.byte()\n",
    "        print(\"predicted == labels:\\n\", predicted == labels)\n",
    "        #sum_of_correct_predicted = torch.sum((predicted == labels).all(1))\n",
    "        sum_of_correct_predicted = torch.sum((predicted == labels))\n",
    "        item = sum_of_correct_predicted.item()\n",
    "        correct_predicted_total += item\n",
    "        \n",
    "    epoch_train_loss = train_loss / train_dataset_size\n",
    "    epoch_train_accuracy = correct_predicted_total / train_dataset_size\n",
    "    \n",
    "    validation_loss, validation_accuracy = validate(hwi_conv_neural_net, validation_loader, criterion)\n",
    "    \n",
    "    epoch_validation_loss = validation_loss / validation_dataset_size\n",
    "    epoch_validation_accuracy = validation_accuracy / validation_dataset_size\n",
    "    \n",
    "    print(\"Epoch {}: train loss {}, train accuracy\"\n",
    "          \" {}, validation loss {}, validation accuracy {}\".format(\n",
    "              epoch + 1,\n",
    "              epoch_train_loss,\n",
    "              epoch_train_accuracy,\n",
    "              epoch_validation_loss,\n",
    "              epoch_validation_accuracy\n",
    "        )\n",
    "    )\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accuracies.append(epoch_train_accuracy)\n",
    "    validation_losses.append(epoch_validation_loss)\n",
    "    validation_accuracies.append(epoch_validation_accuracy)\n",
    "            \n",
    "print(\"Finished Training\")\n",
    "time_elapsed = time.time() - since\n",
    "print(\n",
    "    'Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_losses)), train_losses, label='train')\n",
    "plt.plot(np.arange(len(validation_losses)), validation_losses, label='validation')\n",
    "plt.legend()\n",
    "plt.title(\"loss by epoch\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(len(train_accuracies)), train_accuracies, label='train')\n",
    "plt.plot(np.arange(len(validation_accuracies)), validation_accuracies, label='validation')\n",
    "plt.legend()\n",
    "plt.title(\"accuracy by epoch\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
