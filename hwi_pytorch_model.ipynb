{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import skimage\n",
    "from skimage import transform as skimg_transform\n",
    "#from skimage import io as img_io\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, utils, models\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import load_data\n",
    "import model\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from '/home/uldo/work/kaggle/competitions/Humpback_Whale_Identification/code/model.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.style.use('Solarize_Light2')\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "importlib.reload(load_data)\n",
    "importlib.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SIZE = 100\n",
    "#BATCH_SIZE = 32\n",
    "#BATCH_SIZE = 200\n",
    "#BATCH_SIZE = 128\n",
    "#BATCH_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "#IMAGE_W = 100\n",
    "IMAGE_W = 256\n",
    "#IMAGE_H = 100\n",
    "IMAGE_H = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass HWI_ConvNeuralNet(nn.Module):\\n    \\n    def __init__(self):\\n        super(HWI_ConvNeuralNet, self).__init__()\\n        self.conv1 = nn.Sequential(\\n            nn.Conv2d(3, 6, 5, 1, 2),\\n            nn.ReLU(),\\n            nn.MaxPool2d(kernel_size=2)\\n        )\\n        self.conv2 = nn.Sequential(\\n            nn.Conv2d(6, 12, 5, 1, 2),\\n            nn.ReLU(),\\n            nn.MaxPool2d(kernel_size=2)\\n        )\\n        self.drop_out = nn.Dropout()\\n        self.out1 = nn.Linear(int(12 * IMAGE_W/4 * IMAGE_H/4), 900)\\n        self.out2 = nn.Linear(900, 1)\\n        \\n    def forward(self, x):\\n        x = self.conv1(x)\\n        x = self.conv2(x)\\n        x = x.view(x.size(0), -1)\\n        output = self.drop_out(x)\\n        output = self.out1(x)\\n        output = self.out2(output)\\n        #return output, x\\n        return output[:, 0]\\n        #return F.log_softmax(output, dim=1)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class HWI_ConvNeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HWI_ConvNeuralNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 12, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.out1 = nn.Linear(int(12 * IMAGE_W/4 * IMAGE_H/4), 900)\n",
    "        self.out2 = nn.Linear(900, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.drop_out(x)\n",
    "        output = self.out1(x)\n",
    "        output = self.out2(output)\n",
    "        #return output, x\n",
    "        return output[:, 0]\n",
    "        #return F.log_softmax(output, dim=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HWI_ConvNeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HWI_ConvNeuralNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, 7, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)        \n",
    "        self.pool2 = nn.AvgPool2d(3, 3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 4 * 4 * 16, 1024)\n",
    "        #self.fc1 = nn.Linear(64, 1024)\n",
    "        #self.fc2 = nn.Linear(1024, 5004)\n",
    "        self.fc2 = nn.Linear(1024, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"forward, input x.size(): \", x.size())\n",
    "        x = self.pool(F.relu(self.conv2_bn(self.conv1(x))))\n",
    "        #print(\"forward, x.size() after self.pool(F.relu(self.conv2_bn(self.conv1(x)))): \", x.size())\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        #print(\"forward, x.size() after self.pool2(F.relu(self.conv2(x))): \", x.size())\n",
    "        x = x.view(-1, 64 * 4 * 4 * 16)\n",
    "        #print(\"forward, x.size() after x.view(-1, 64 * 4 * 4 * 16): \", x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(\"forward, x.size() after F.relu(self.fc1(x)): \", x.size())\n",
    "        x = self.dropout(x)\n",
    "        #print(\"forward, x.size() after self.dropout(x): \", x.size())\n",
    "        x = self.fc2(x)\n",
    "        #print(\"forward, x.size() after self.fc2(x): \", x.size())\n",
    "        #x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        #print(\"forward, x.size() after self.sigmoid(x): \", x.size())\n",
    "        return x       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HWI_ConvNeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HWI_ConvNeuralNet, self).__init__()\n",
    "        '''\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 12, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.out1 = nn.Linear(int(12 * IMAGE_W/4 * IMAGE_H/4), 900)\n",
    "        self.out2 = nn.Linear(900, 1)\n",
    "        '''\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=9, stride=2, padding=4)\n",
    "        #self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2)\n",
    "        #self.fc1 = nn.Linear(int(IMAGE_H * IMAGE_W 131072 x 16), 1024)\n",
    "        #self.fc1 = nn.Linear(524288 * 64, 1024)\n",
    "        self.fc1 = nn.Linear(32768 * 16, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 32)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        #forward, input x.size():  torch.Size([64, 3, 256, 256])\n",
    "        #forward, after F.relu(self.conv1(x)) x.size():  torch.Size([64, 32, 128, 128])\n",
    "        #forward, after self.pool(x) x.size():  torch.Size([64, 32, 64, 64])\n",
    "        #forward, after F.relu(self.conv2(x)) x.size():  torch.Size([64, 64, 32, 32])\n",
    "        #forward, after self.pool(x) x.size():  torch.Size([64, 64, 16, 16])\n",
    "        #forward, after x.view(-1, 65536 * 16) x.size():  torch.Size([1, 1048576])\n",
    "        #forward, after F.relu(self.fc1(x)) x.size():  torch.Size([1, 1024])\n",
    "        #forward, after self.dropout x.size():  torch.Size([1, 1024])\n",
    "        #forward, after self.fc2 x.size():  torch.Size([1, 1])\n",
    "        #forward, after self.sigmoid(x) x.size():  torch.Size([1, 1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.drop_out(x)\n",
    "        output = self.out1(x)\n",
    "        output = self.out2(output)\n",
    "        #return output, x\n",
    "        return output[:, 0]\n",
    "        '''\n",
    "        #print(\"forward, input x.size(): \", x.size())\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(\"forward, after F.relu(self.conv1(x)) x.size(): \", x.size())\n",
    "        x = self.pool(x)\n",
    "        #print(\"forward, after self.pool(x) x.size(): \", x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(\"forward, after F.relu(self.conv2(x)) x.size(): \", x.size())\n",
    "        x = self.pool(x)     \n",
    "        #print(\"forward, after self.pool(x) x.size(): \", x.size())  \n",
    "        x = x.view(-1, 32768 * 16)\n",
    "        #print(\"forward, after x.view(-1, 65536 * 16) x.size(): \", x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(\"forward, after F.relu(self.fc1(x)) x.size(): \", x.size())\n",
    "        x = self.dropout(x)\n",
    "        #print(\"forward, after self.dropout x.size(): \", x.size())\n",
    "        x = self.fc2(x)\n",
    "        #print(\"forward, after self.fc2 x.size(): \", x.size())\n",
    "        x = self.sigmoid(x)\n",
    "        #print(\"forward, after self.sigmoid(x) x.size(): \", x.size())\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        orig_height, orig_width = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if orig_height > orig_width:\n",
    "                new_height, new_width = self.output_size * orig_height / orig_width, self.output_size\n",
    "            else:\n",
    "                new_height, new_width = self.output_size, self.output_size * orig_width / orig_height\n",
    "        else:\n",
    "            new_height, new_width = self.output_size\n",
    "\n",
    "        new_height, new_width = int(new_height), int(new_width)\n",
    "\n",
    "        img = skimg_transform.resize(image, (new_height, new_width))\n",
    "\n",
    "        return {'image': img, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifyRescale(object):\n",
    "    \n",
    "    def __init__(self, output_size=128):\n",
    "        assert isinstance(output_size, int)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        img = skimg_transform.resize(image, (self.output_size, self.output_size))\n",
    "\n",
    "        return {'image': img, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        orig_height, orig_width = image.shape[:2]\n",
    "        new_height, new_width = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, orig_height - new_height)\n",
    "        left = np.random.randint(0, orig_width - new_width)\n",
    "\n",
    "        image = image[top: top + new_height, left: left + new_width]\n",
    "\n",
    "        return {'image': image, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __init__(self, image_size=128):\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        \n",
    "        \"\"\" The original code didn't expect gray scale images \"\"\"\n",
    "        \n",
    "        gray_scale_image = torch.zeros(\n",
    "            [self.image_size, self.image_size]\n",
    "        ).shape == image.shape\n",
    "        if gray_scale_image:\n",
    "            image = np.stack((image,) * 3, axis=-1)\n",
    "        \n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image), 'label': torch.tensor(label, dtype=torch.uint8)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_whale_batch(sample_batched):\n",
    "    \"\"\"Show whales for a batch of samples.\"\"\"\n",
    "    images_batch = sample_batched['image']\n",
    "    labels_batch = sample_batched['label']\n",
    "    batch_size = len(images_batch)\n",
    "    im_size = images_batch.size(2)\n",
    "\n",
    "    grid = utils.make_grid(images_batch)\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        plt.title('Batch from dataloader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(submodule):\n",
    "    if type(submodule) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(submodule.weight)\n",
    "        submodule.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders(dataset, valid_train_ratio=0.6):\n",
    "    dataset_size = len(dataset)\n",
    "    print(\"dataset_size: \", dataset_size)\n",
    "\n",
    "    validation_subset_size = int(dataset_size * (1 - valid_train_ratio))\n",
    "    print(\"validation_subset_size: \", validation_subset_size)\n",
    "\n",
    "    indices = list(range(dataset_size))\n",
    "    validation_indices = np.random.choice(indices, size=validation_subset_size, replace=False)\n",
    "    train_indices = list(set(indices) - set(validation_indices))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    validation_sampler = SubsetRandomSampler(validation_indices)\n",
    "    \n",
    "    dataset_sizes = {\n",
    "            'train': len(train_indices),\n",
    "            'validation': len(validation_indices)\n",
    "        }\n",
    "\n",
    "    #train_loader = data.DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=1, sampler=train_sampler, pin_memory=True)\n",
    "    train_loader = data.DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=1, sampler=train_sampler)\n",
    "    #validation_loader = data.DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=1, sampler=validation_sampler, pin_memory=True)\n",
    "    validation_loader = data.DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=1, sampler=validation_sampler)\n",
    "    loaders = {\n",
    "            'train': train_loader,\n",
    "            'validation': validation_loader\n",
    "        }\n",
    "\n",
    "    return loaders, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfold_batch(batch):\n",
    "    return batch['image'], batch['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch_train(model, data_loader, criterion, optimizer):\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    total_loss = 0.0\n",
    "    correct_predicted_total = 0.0\n",
    "    \n",
    "    for i, data_batch in enumerate(data_loader, 0):\n",
    "        #print(\"one_epoch_model_process, len(data_batch): \", len(data_batch))\n",
    "        #print(\"one_epoch_model_process, type(data_batch): \", type(data_batch))        \n",
    "        \n",
    "        inputs, labels = unfold_batch(data_batch)\n",
    "        #print(\"one_epoch_model_process, inputs.size(): \", inputs.size())\n",
    "        if inputs.size()[0] == BATCH_SIZE:\n",
    "            \n",
    "            #print(\"inputs: \", inputs)\n",
    "            #print(\"labels: \", labels)\n",
    "        \n",
    "            inputs = inputs.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            \n",
    "            outputs = hwi_conv_neural_net(inputs)\n",
    "\n",
    "            #print(\"outputs.size():\\n\", outputs.size())\n",
    "            #print(\"labels.size():\\n\", labels.size())\n",
    "            #print(\"outputs:\\n\", outputs)\n",
    "            #print(\"outputs[:, 0]:\\n\", outputs[:, 0])\n",
    "            #print(\"labels:\\n\", labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "            predicted = outputs > 0\n",
    "            #print(\"type(predicted): \", type(predicted))\n",
    "            #print(\"predicted:\\n\", predicted)\n",
    "\n",
    "            #total += labels.size(0)\n",
    "            labels = labels.data.byte()\n",
    "            #print(\"predicted == labels:\\n\", predicted == labels)\n",
    "            #sum_of_correct_predicted = torch.sum((predicted == labels).all(1))\n",
    "            sum_of_correct_predicted = torch.sum((predicted == labels))\n",
    "            item = sum_of_correct_predicted.item()\n",
    "            correct_predicted_total += item\n",
    "        \n",
    "    accuracy = correct_predicted_total\n",
    "    \n",
    "    #epoch_train_loss = total_loss / train_dataset_size\n",
    "    #epoch_train_accuracy = correct_predicted_total / train_dataset_size\n",
    "    return (total_loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, validation_loader, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        correct_predicted_total = 0.0\n",
    "        total_loss = 0.0\n",
    "        #total = 0.0\n",
    "        \n",
    "        for data_batch in validation_loader:\n",
    "            inputs, labels = unfold_batch(data_batch)\n",
    "\n",
    "            inputs = inputs.to(device, dtype=torch.float)\n",
    "            #labels_as_float = labels.to(device, dtype=torch.float)\n",
    "            #labels = labels.to(device, dtype=torch.uint8)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            #loss = criterion(outputs, labels_as_float)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            predicted = outputs > 0\n",
    "\n",
    "            #total += labels.size(0)\n",
    "            labels = labels.data.byte()\n",
    "            #sum_of_correct_predicted = torch.sum((predicted == labels).all(1))\n",
    "            sum_of_correct_predicted = torch.sum((predicted == labels))\n",
    "            item = sum_of_correct_predicted.item()\n",
    "\n",
    "            correct_predicted_total += item\n",
    "\n",
    "        #accuracy = correct_predicted_total / total\n",
    "        accuracy = correct_predicted_total\n",
    "        \n",
    "\n",
    "    return (total_loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch_validate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        correct_predicted_total = 0.0\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for data_batch in data_loader:\n",
    "            inputs, labels = unfold_batch(data_batch)\n",
    "            if inputs.size()[0] == BATCH_SIZE:\n",
    "\n",
    "                inputs = inputs.to(device, dtype=torch.float)\n",
    "                labels = labels.to(device, dtype=torch.float)\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "                predicted = outputs > 0\n",
    "            \n",
    "                labels = labels.data.byte()\n",
    "                sum_of_correct_predicted = torch.sum((predicted == labels))\n",
    "                item = sum_of_correct_predicted.item()\n",
    "\n",
    "                correct_predicted_total += item\n",
    "\n",
    "        accuracy = correct_predicted_total        \n",
    "\n",
    "    return (total_loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch_model_process(model, loader, criterion, optimizer=None):\n",
    "    accuracy = 0.0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for data_batch in loader:\n",
    "        print(\"one_epoch_model_process, len(data_batch): \", len(data_batch))\n",
    "        print(\"one_epoch_model_process, type(data_batch): \", type(data_batch))\n",
    "        if len(data_batch) == BATCH_SIZE:\n",
    "            inputs, labels = unfold_batch(data_batch)\n",
    "            inputs = inputs.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "            outputs = model(inputs)[:, 0]\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            if optimizer:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            predicted = outputs > 0\n",
    "\n",
    "            labels = labels.data.byte()\n",
    "            #torch_sum_of_correct_predicted = torch.sum((predicted == labels).all(1))\n",
    "            torch_sum_of_correct_predicted = torch.sum((predicted == labels))\n",
    "            correct_predicted = torch_sum_of_correct_predicted.item()\n",
    "\n",
    "    accuracy += correct_predicted        \n",
    "\n",
    "    return (total_loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_of_epoch, model, dataset_loaders, dataset_sizes, criterion, optimizer):\n",
    "    torch.cuda.empty_cache()\n",
    "    since = time.time()\n",
    "    \n",
    "    train_loader = dataset_loaders['train']\n",
    "    validation_loader = dataset_loaders['validation']\n",
    "    train_dataset_size = dataset_sizes['train']\n",
    "    validation_dataset_size = dataset_sizes['validation']\n",
    "    \n",
    "    best_model_accuracy = 0.0\n",
    "    best_model_weights = model.state_dict\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    validation_losses = []\n",
    "    validation_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_of_epoch):\n",
    "        \n",
    "        train_loss, train_accuracy = one_epoch_train(model, train_loader, criterion, optimizer)\n",
    "        train_losses.append(train_loss / train_dataset_size)\n",
    "        train_accuracies.append(train_accuracy / train_dataset_size)\n",
    "        \n",
    "        validation_loss, validation_accuracy = one_epoch_validate(model, validation_loader, criterion)\n",
    "        validation_losses.append(validation_loss / validation_dataset_size)\n",
    "        validation_accuracies.append(validation_accuracy / validation_dataset_size)\n",
    "        \n",
    "        print(\"Epoch {}: train loss {}, train accuracy\"\n",
    "          \" {}, validation loss {}, validation accuracy {}\".format(\n",
    "              epoch + 1,\n",
    "              train_loss / train_dataset_size,\n",
    "              train_accuracy / train_dataset_size,\n",
    "              validation_loss / validation_dataset_size,\n",
    "              validation_accuracy / validation_dataset_size\n",
    "            )\n",
    "        )\n",
    "    print(\"Finished Training\")\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\n",
    "            'Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60\n",
    "        )\n",
    "    )\n",
    "    return train_losses, validation_losses, train_accuracies, validation_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, batch):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    #inputs = batch\n",
    "    #inputs = inputs.to(device, dtype=torch.float)\n",
    "    inputs = batch.to(device, dtype=torch.float)\n",
    "    outputs = model(inputs)\n",
    "    #return outputs[0].cpu()\n",
    "    return outputs.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, full=True, name='model'):\n",
    "    if not full:\n",
    "        torch.save(model.state_dict(), '{}_params.pkl'.format(name))\n",
    "    else:\n",
    "        torch.save(model, '{}.pkl'.format(name))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_model(name='model'):\n",
    "    return torch.load('{}.pkl'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data.load_text_data('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_two_classes = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_two_classes.loc[train_df_two_classes['Id'] != 'new_whale', 'Id'] = 'not_new_whale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000e88ab.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001f9222.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00029d126.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00050a15a.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0005c1ef8.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0006e997e.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000a6daec.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000f0f2bf.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0016b897a.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>001c1ac5f.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>001cae55b.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>001d7450c.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00200e115.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00245a598.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>002b4615d.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>002f99f01.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00355ff28.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00357e37a.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>003795857.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0041880bf.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0043da555.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>00442c882.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>00464ff65.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>004775679.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>004ae9e26.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>004c0f43b.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>004e8ad5b.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>004f87702.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0050ef29d.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>00514c876.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0052ce2f5.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>00537ec91.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>00570db6b.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>005ce3100.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>00600ce17.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>006017ddf.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0060f764a.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>006500b3d.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>006506edf.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0067b3a20.jpg</td>\n",
       "      <td>not_new_whale</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Image             Id\n",
       "0   0000e88ab.jpg  not_new_whale\n",
       "1   0001f9222.jpg  not_new_whale\n",
       "2   00029d126.jpg  not_new_whale\n",
       "3   00050a15a.jpg      new_whale\n",
       "4   0005c1ef8.jpg      new_whale\n",
       "5   0006e997e.jpg      new_whale\n",
       "6   000a6daec.jpg  not_new_whale\n",
       "7   000f0f2bf.jpg      new_whale\n",
       "8   0016b897a.jpg  not_new_whale\n",
       "9   001c1ac5f.jpg  not_new_whale\n",
       "10  001cae55b.jpg  not_new_whale\n",
       "11  001d7450c.jpg      new_whale\n",
       "12  00200e115.jpg      new_whale\n",
       "13  00245a598.jpg      new_whale\n",
       "14  002b4615d.jpg      new_whale\n",
       "15  002f99f01.jpg      new_whale\n",
       "16  00355ff28.jpg  not_new_whale\n",
       "17  00357e37a.jpg  not_new_whale\n",
       "18  003795857.jpg      new_whale\n",
       "19  0041880bf.jpg      new_whale\n",
       "20  0043da555.jpg      new_whale\n",
       "21  00442c882.jpg  not_new_whale\n",
       "22  00464ff65.jpg      new_whale\n",
       "23  004775679.jpg  not_new_whale\n",
       "24  004ae9e26.jpg      new_whale\n",
       "25  004c0f43b.jpg      new_whale\n",
       "26  004e8ad5b.jpg  not_new_whale\n",
       "27  004f87702.jpg  not_new_whale\n",
       "28  0050ef29d.jpg  not_new_whale\n",
       "29  00514c876.jpg  not_new_whale\n",
       "30  0052ce2f5.jpg  not_new_whale\n",
       "31  00537ec91.jpg  not_new_whale\n",
       "32  00570db6b.jpg  not_new_whale\n",
       "33  005ce3100.jpg      new_whale\n",
       "34  00600ce17.jpg      new_whale\n",
       "35  006017ddf.jpg  not_new_whale\n",
       "36  0060f764a.jpg  not_new_whale\n",
       "37  006500b3d.jpg  not_new_whale\n",
       "38  006506edf.jpg      new_whale\n",
       "39  0067b3a20.jpg  not_new_whale"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_two_classes.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimage_size = 128\\ndataset = load_data.HumpbackWhalesDataset(\\n    train_df_two_classes,\\n    #train_df,\\n    #transform=load_data.transforms.ToTensor()\\n    #transform=ToTensor()\\n    transform=transforms.Compose(\\n        [\\n            #Rescale(int(image_size*1.25)),\\n            Rescale(int(image_size)),\\n            #RandomCrop(image_size),\\n            UnifyRescale(int(image_size)),\\n            ToTensor()\\n        ]\\n    )\\n)\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "image_size = 128\n",
    "dataset = load_data.HumpbackWhalesDataset(\n",
    "    train_df_two_classes,\n",
    "    #train_df,\n",
    "    #transform=load_data.transforms.ToTensor()\n",
    "    #transform=ToTensor()\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            #Rescale(int(image_size*1.25)),\n",
    "            Rescale(int(image_size)),\n",
    "            #RandomCrop(image_size),\n",
    "            UnifyRescale(int(image_size)),\n",
    "            ToTensor()\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_size = 128\n",
    "#image_size = 100\n",
    "dataset = load_data.HumpbackWhalesDataset(\n",
    "    train_df_two_classes,\n",
    "    #train_df,\n",
    "    #transform=load_data.transforms.ToTensor()\n",
    "    #transform=ToTensor()\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((IMAGE_W, IMAGE_H)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_size:  25361\n",
      "validation_subset_size:  10144\n"
     ]
    }
   ],
   "source": [
    "dataset_loaders, dataset_sizes = prepare_loaders(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = dataset_loaders['train']\n",
    "validation_loader = dataset_loaders['validation']\n",
    "train_dataset_size = dataset_sizes['train']\n",
    "validation_dataset_size = dataset_sizes['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15217\n",
      "10144\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_size)\n",
    "print(validation_dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f6bcc82ecc0>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader._DataLoaderIter'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[[ 0.7419,  0.7762,  0.7762,  ...,  0.4337,  0.3994,  0.3309],\n",
      "          [ 0.8447,  0.8789,  0.8789,  ...,  0.4851,  0.4508,  0.3823],\n",
      "          [ 0.8276,  0.8789,  0.8789,  ...,  0.4166,  0.4508,  0.4337],\n",
      "          ...,\n",
      "          [ 0.6221,  0.5707,  0.8104,  ...,  0.3652,  0.3652,  0.3309],\n",
      "          [ 0.5364,  0.3652,  0.5707,  ...,  0.2453,  0.2453,  0.1939],\n",
      "          [ 0.2624,  0.1768,  0.2967,  ...,  0.1597,  0.1426,  0.0912]],\n",
      "\n",
      "         [[ 1.1856,  1.2206,  1.2206,  ...,  0.6954,  0.6604,  0.6078],\n",
      "          [ 1.2906,  1.3256,  1.3256,  ...,  0.7479,  0.7129,  0.6429],\n",
      "          [ 1.2731,  1.3256,  1.3256,  ...,  0.6779,  0.7129,  0.6954],\n",
      "          ...,\n",
      "          [ 0.9230,  0.8354,  1.0630,  ...,  0.6254,  0.6429,  0.5903],\n",
      "          [ 0.8354,  0.6604,  0.8529,  ...,  0.5203,  0.5378,  0.4853],\n",
      "          [ 0.5553,  0.4853,  0.5728,  ...,  0.4503,  0.4328,  0.3803]],\n",
      "\n",
      "         [[ 1.7163,  1.7860,  1.7860,  ...,  1.1934,  1.1585,  1.0888],\n",
      "          [ 1.8208,  1.8905,  1.8905,  ...,  1.2457,  1.2108,  1.1411],\n",
      "          [ 1.8208,  1.8905,  1.8905,  ...,  1.1759,  1.2108,  1.1934],\n",
      "          ...,\n",
      "          [ 1.3328,  1.3328,  1.5420,  ...,  1.1411,  1.1411,  1.0888],\n",
      "          [ 1.2457,  1.1411,  1.3502,  ...,  1.0365,  1.0191,  0.9668],\n",
      "          [ 0.9494,  0.9145,  1.0714,  ...,  0.9494,  0.9145,  0.8622]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1804,  2.1633,  2.0092,  ...,  1.2214,  1.1015,  1.1015],\n",
      "          [ 2.0948,  2.0263,  1.8037,  ...,  1.7352,  1.7694,  1.6495],\n",
      "          [ 2.0263,  2.0434,  2.0092,  ...,  1.4783,  1.6324,  1.8037],\n",
      "          ...,\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2318,  2.2489,  2.2489],\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2318,  2.2489,  2.2489]],\n",
      "\n",
      "         [[ 2.3585,  2.3410,  2.1835,  ...,  1.3782,  1.2556,  1.2556],\n",
      "          [ 2.2710,  2.2010,  1.9734,  ...,  1.9034,  1.9384,  1.8158],\n",
      "          [ 2.2010,  2.2185,  2.1835,  ...,  1.6408,  1.7983,  1.9734],\n",
      "          ...,\n",
      "          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          [ 2.4286,  2.4286,  2.4286,  ...,  2.4111,  2.4286,  2.4286],\n",
      "          [ 2.4286,  2.4286,  2.4286,  ...,  2.4111,  2.4286,  2.4286]],\n",
      "\n",
      "         [[ 2.5703,  2.5529,  2.3960,  ...,  1.5942,  1.4722,  1.4722],\n",
      "          [ 2.4831,  2.4134,  2.1868,  ...,  2.1171,  2.1520,  2.0300],\n",
      "          [ 2.4134,  2.4308,  2.3960,  ...,  1.8557,  2.0125,  2.1868],\n",
      "          ...,\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6226,  2.6400,  2.6400],\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6226,  2.6400,  2.6400]]],\n",
      "\n",
      "\n",
      "        [[[-0.3369, -0.2856, -0.3369,  ..., -0.3198, -0.3369, -0.3027],\n",
      "          [-0.2684, -0.3027, -0.2684,  ..., -0.3027, -0.3369, -0.3198],\n",
      "          [-0.2684, -0.3027, -0.2513,  ..., -0.2684, -0.3541, -0.3541],\n",
      "          ...,\n",
      "          [-0.9877, -0.9877, -1.0904,  ..., -0.9534, -0.7479, -0.8335],\n",
      "          [-0.9877, -0.9020, -0.8678,  ..., -1.0048, -0.9020, -0.8507],\n",
      "          [-1.0733, -1.0390, -0.8164,  ..., -0.7993, -0.8678, -0.8678]],\n",
      "\n",
      "         [[ 0.3627,  0.4153,  0.3627,  ...,  0.3627,  0.3452,  0.3803],\n",
      "          [ 0.4328,  0.3978,  0.4328,  ...,  0.3803,  0.3452,  0.3627],\n",
      "          [ 0.4328,  0.3978,  0.4503,  ...,  0.4153,  0.3277,  0.3277],\n",
      "          ...,\n",
      "          [-0.3725, -0.3550, -0.4251,  ..., -0.2850, -0.1275, -0.2150],\n",
      "          [-0.3725, -0.2675, -0.1975,  ..., -0.3550, -0.2850, -0.2325],\n",
      "          [-0.4601, -0.4076, -0.1450,  ..., -0.1450, -0.2500, -0.2500]],\n",
      "\n",
      "         [[ 1.1934,  1.2457,  1.1934,  ...,  1.2457,  1.2282,  1.2631],\n",
      "          [ 1.2631,  1.2282,  1.2631,  ...,  1.2631,  1.2282,  1.2457],\n",
      "          [ 1.2631,  1.2282,  1.2805,  ...,  1.2980,  1.2108,  1.2108],\n",
      "          ...,\n",
      "          [ 0.5485,  0.5659,  0.4962,  ...,  0.6531,  0.8274,  0.7228],\n",
      "          [ 0.5485,  0.6531,  0.7228,  ...,  0.5834,  0.6705,  0.6879],\n",
      "          [ 0.4614,  0.4962,  0.7576,  ...,  0.7925,  0.6705,  0.6705]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.7137, -0.7308, -0.7308,  ..., -0.8849, -0.8678, -0.8507],\n",
      "          [-0.6623, -0.6965, -0.6281,  ..., -0.6965, -0.6623, -0.6623],\n",
      "          [-0.7308, -0.7822, -0.6623,  ..., -0.6109, -0.5424, -0.5253],\n",
      "          ...,\n",
      "          [-1.5014, -1.1418, -1.4329,  ..., -1.0048, -1.0562, -1.3815],\n",
      "          [-1.7412, -1.5357, -1.6384,  ..., -1.0219, -1.0904, -1.4329],\n",
      "          [-1.9295, -1.8439, -1.8439,  ..., -0.7479, -1.5185, -1.7583]],\n",
      "\n",
      "         [[ 0.1352,  0.1702,  0.2227,  ...,  0.0126,  0.0476,  0.0301],\n",
      "          [ 0.2227,  0.2227,  0.3102,  ...,  0.2052,  0.2577,  0.2052],\n",
      "          [ 0.1877,  0.1702,  0.2752,  ...,  0.2752,  0.3627,  0.3277],\n",
      "          ...,\n",
      "          [-0.9153, -0.5301, -0.7752,  ..., -0.2675, -0.3025, -0.6176],\n",
      "          [-1.1604, -0.9328, -1.0028,  ..., -0.3200, -0.3901, -0.7402],\n",
      "          [-1.3529, -1.2654, -1.2304,  ..., -0.0749, -0.8978, -1.1429]],\n",
      "\n",
      "         [[ 1.3502,  1.4200,  1.4374,  ...,  1.1585,  1.2282,  1.1585],\n",
      "          [ 1.4200,  1.4722,  1.5420,  ...,  1.4548,  1.5420,  1.4548],\n",
      "          [ 1.4025,  1.4200,  1.5071,  ...,  1.5768,  1.6814,  1.6117],\n",
      "          ...,\n",
      "          [ 0.0082,  0.3742,  0.1476,  ...,  0.8448,  0.8274,  0.5485],\n",
      "          [-0.2358, -0.0267, -0.0964,  ...,  0.7228,  0.7402,  0.4091],\n",
      "          [-0.4275, -0.3404, -0.3055,  ...,  0.8797,  0.2522,  0.0082]]],\n",
      "\n",
      "\n",
      "        [[[ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],\n",
      "          ...,\n",
      "          [ 2.2489,  2.2489,  2.2489,  ...,  2.2318,  2.2318,  2.2318],\n",
      "          [ 2.2318,  2.2489,  2.2489,  ...,  2.2318,  2.2489,  2.2318],\n",
      "          [ 2.2318,  2.2318,  2.2147,  ...,  2.2318,  2.2489,  2.2489]],\n",
      "\n",
      "         [[ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],\n",
      "          ...,\n",
      "          [ 2.4286,  2.4286,  2.4286,  ...,  2.4111,  2.4111,  2.4111],\n",
      "          [ 2.4111,  2.4286,  2.4286,  ...,  2.4111,  2.4286,  2.4111],\n",
      "          [ 2.4111,  2.4111,  2.3936,  ...,  2.4111,  2.4286,  2.4286]],\n",
      "\n",
      "         [[ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "          ...,\n",
      "          [ 2.6400,  2.6400,  2.6400,  ...,  2.6226,  2.6226,  2.6226],\n",
      "          [ 2.6226,  2.6400,  2.6400,  ...,  2.6226,  2.6400,  2.6226],\n",
      "          [ 2.6226,  2.6226,  2.6051,  ...,  2.6226,  2.6400,  2.6400]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7762,  0.5707,  0.6221,  ...,  0.7933,  0.6563,  0.4679],\n",
      "          [ 0.7419,  0.5364,  0.5022,  ...,  0.8276,  0.6734,  0.4508],\n",
      "          [ 0.5536,  0.4679,  0.4508,  ...,  0.7933,  0.6906,  0.5022],\n",
      "          ...,\n",
      "          [ 1.8379,  1.6153,  1.5125,  ...,  0.9132,  0.7248,  0.7248],\n",
      "          [ 1.7523,  1.5639,  1.5468,  ...,  0.9646,  0.7762,  0.7419],\n",
      "          [ 1.5982,  1.5297,  1.5639,  ...,  0.7419,  0.6221,  0.6221]],\n",
      "\n",
      "         [[ 0.9230,  0.7129,  0.7654,  ...,  0.9405,  0.8004,  0.6078],\n",
      "          [ 0.8880,  0.6779,  0.6429,  ...,  0.9755,  0.8179,  0.5903],\n",
      "          [ 0.6954,  0.6078,  0.5903,  ...,  0.9405,  0.8354,  0.6429],\n",
      "          ...,\n",
      "          [ 2.0084,  1.7808,  1.6758,  ...,  1.0630,  0.8704,  0.8704],\n",
      "          [ 1.9209,  1.7283,  1.7108,  ...,  1.1155,  0.9230,  0.8880],\n",
      "          [ 1.7633,  1.6933,  1.7283,  ...,  0.8880,  0.7654,  0.7654]],\n",
      "\n",
      "         [[ 1.1411,  0.9319,  0.9842,  ...,  1.1585,  1.0191,  0.8274],\n",
      "          [ 1.1062,  0.8971,  0.8622,  ...,  1.1934,  1.0365,  0.8099],\n",
      "          [ 0.9145,  0.8274,  0.8099,  ...,  1.1585,  1.0539,  0.8622],\n",
      "          ...,\n",
      "          [ 2.2217,  1.9951,  1.8905,  ...,  1.2805,  1.0888,  1.0888],\n",
      "          [ 2.1346,  1.9428,  1.9254,  ...,  1.3328,  1.1411,  1.1062],\n",
      "          [ 1.9777,  1.9080,  1.9428,  ...,  1.1062,  0.9842,  0.9842]]]]), 'label': tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "#images, labels = train_iter.next()\n",
    "sample = train_iter.next()\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i_batch, sample_batched in enumerate(train_loader):\\n    print(i_batch, sample_batched['image'].size(),\\n          sample_batched['label'])\\n    # observe 4th batch and stop.\\n    if i_batch == 0:\\n        plt.figure(figsize=(24, 24))\\n        show_whale_batch(sample_batched)\\n        plt.axis('off')\\n        #plt.ioff()\\n        plt.show()\\n        break\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    print(i_batch, sample_batched['image'].size(),\n",
    "          sample_batched['label'])\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 0:\n",
    "        plt.figure(figsize=(24, 24))\n",
    "        show_whale_batch(sample_batched)\n",
    "        plt.axis('off')\n",
    "        #plt.ioff()\n",
    "        plt.show()\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'not_new_whale': 0, 'new_whale': 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HWI_ConvNeuralNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(9, 9), stride=(2, 2), padding=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (fc1): Linear(in_features=524288, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hwi_conv_neural_net = HWI_ConvNeuralNet()\n",
    "hwi_conv_neural_net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HWI_ConvNeuralNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(9, 9), stride=(2, 2), padding=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (fc1): Linear(in_features=524288, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hwi_conv_neural_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(hwi_conv_neural_net.parameters(), lr=0.001, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_epoch = 12\n",
    "#train_losses = []\n",
    "#train_accuracies = []\n",
    "#validation_losses = []\n",
    "#validation_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.cuda.empty_cache()\\nsince = time.time()\\nfor epoch in range(num_of_epoch):\\n    epoch_train_accuracy = 0.0\\n    epoch_train_loss = 0.0\\n    correct_predicted_total = 0.0\\n    train_loss = 0.0\\n    #total_loss = 0.0\\n    #total = 0.0\\n    for i, data_batch in enumerate(train_loader, 0):\\n        \\n        inputs = data_batch[\\'image\\']\\n        labels = data_batch[\\'label\\']\\n        print(\"inputs: \", inputs)\\n        print(\"labels: \", labels)\\n        inputs = inputs.to(device, dtype=torch.float)\\n        labels = labels.to(device, dtype=torch.float)\\n        optimizer.zero_grad()\\n        \\n        outputs = hwi_conv_neural_net(inputs)\\n\\n        print(\"outputs.size():\\n\", outputs.size())\\n        print(\"labels.size():\\n\", labels.size())\\n        print(\"outputs:\\n\", outputs)\\n        #print(\"outputs[:, 0]:\\n\", outputs[:, 0])\\n        print(\"labels:\\n\", labels)\\n        loss = criterion(outputs, labels)\\n        loss.backward()\\n        optimizer.step()\\n        \\n        train_loss += loss.item() * inputs.size(0)\\n        \\n        predicted = outputs > 0\\n        print(\"type(predicted): \", type(predicted))\\n        print(\"predicted:\\n\", predicted)\\n\\n        #total += labels.size(0)\\n        labels = labels.data.byte()\\n        print(\"predicted == labels:\\n\", predicted == labels)\\n        #sum_of_correct_predicted = torch.sum((predicted == labels).all(1))\\n        sum_of_correct_predicted = torch.sum((predicted == labels))\\n        item = sum_of_correct_predicted.item()\\n        correct_predicted_total += item\\n        \\n    epoch_train_loss = train_loss / train_dataset_size\\n    epoch_train_accuracy = correct_predicted_total / train_dataset_size\\n    \\n    validation_loss, validation_accuracy = validate(hwi_conv_neural_net, validation_loader, criterion)\\n    \\n    epoch_validation_loss = validation_loss / validation_dataset_size\\n    epoch_validation_accuracy = validation_accuracy / validation_dataset_size\\n    \\n    print(\"Epoch {}: train loss {}, train accuracy\"\\n          \" {}, validation loss {}, validation accuracy {}\".format(\\n              epoch + 1,\\n              epoch_train_loss,\\n              epoch_train_accuracy,\\n              epoch_validation_loss,\\n              epoch_validation_accuracy\\n        )\\n    )\\n    train_losses.append(epoch_train_loss)\\n    train_accuracies.append(epoch_train_accuracy)\\n    validation_losses.append(epoch_validation_loss)\\n    validation_accuracies.append(epoch_validation_accuracy)\\n            \\nprint(\"Finished Training\")\\ntime_elapsed = time.time() - since\\nprint(\\n    \\'Training complete in {:.0f}m {:.0f}s\\'.format(\\n        time_elapsed // 60, time_elapsed % 60\\n    )\\n)\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "torch.cuda.empty_cache()\n",
    "since = time.time()\n",
    "for epoch in range(num_of_epoch):\n",
    "    epoch_train_accuracy = 0.0\n",
    "    epoch_train_loss = 0.0\n",
    "    correct_predicted_total = 0.0\n",
    "    train_loss = 0.0\n",
    "    #total_loss = 0.0\n",
    "    #total = 0.0\n",
    "    for i, data_batch in enumerate(train_loader, 0):\n",
    "        \n",
    "        inputs = data_batch['image']\n",
    "        labels = data_batch['label']\n",
    "        print(\"inputs: \", inputs)\n",
    "        print(\"labels: \", labels)\n",
    "        inputs = inputs.to(device, dtype=torch.float)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = hwi_conv_neural_net(inputs)\n",
    "\n",
    "        print(\"outputs.size():\\n\", outputs.size())\n",
    "        print(\"labels.size():\\n\", labels.size())\n",
    "        print(\"outputs:\\n\", outputs)\n",
    "        #print(\"outputs[:, 0]:\\n\", outputs[:, 0])\n",
    "        print(\"labels:\\n\", labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        predicted = outputs > 0\n",
    "        print(\"type(predicted): \", type(predicted))\n",
    "        print(\"predicted:\\n\", predicted)\n",
    "\n",
    "        #total += labels.size(0)\n",
    "        labels = labels.data.byte()\n",
    "        print(\"predicted == labels:\\n\", predicted == labels)\n",
    "        #sum_of_correct_predicted = torch.sum((predicted == labels).all(1))\n",
    "        sum_of_correct_predicted = torch.sum((predicted == labels))\n",
    "        item = sum_of_correct_predicted.item()\n",
    "        correct_predicted_total += item\n",
    "        \n",
    "    epoch_train_loss = train_loss / train_dataset_size\n",
    "    epoch_train_accuracy = correct_predicted_total / train_dataset_size\n",
    "    \n",
    "    validation_loss, validation_accuracy = validate(hwi_conv_neural_net, validation_loader, criterion)\n",
    "    \n",
    "    epoch_validation_loss = validation_loss / validation_dataset_size\n",
    "    epoch_validation_accuracy = validation_accuracy / validation_dataset_size\n",
    "    \n",
    "    print(\"Epoch {}: train loss {}, train accuracy\"\n",
    "          \" {}, validation loss {}, validation accuracy {}\".format(\n",
    "              epoch + 1,\n",
    "              epoch_train_loss,\n",
    "              epoch_train_accuracy,\n",
    "              epoch_validation_loss,\n",
    "              epoch_validation_accuracy\n",
    "        )\n",
    "    )\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accuracies.append(epoch_train_accuracy)\n",
    "    validation_losses.append(epoch_validation_loss)\n",
    "    validation_accuracies.append(epoch_validation_accuracy)\n",
    "            \n",
    "print(\"Finished Training\")\n",
    "time_elapsed = time.time() - since\n",
    "print(\n",
    "    'Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60\n",
    "    )\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/uldo/.torch/models/resnet18-5c106cde.pth\n",
      "100%|| 46827520/46827520 [00:04<00:00, 10859971.70it/s]\n"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_num = resnet18.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.fc = nn.Linear(features_num, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.770180433410817, train accuracy 0.37346388907143324, validation loss 0.6710197983856081, validation accuracy 0.3914629337539432\n",
      "Epoch 2: train loss 0.6627301558751334, train accuracy 0.37392390090030886, validation loss 0.6718500892070566, validation accuracy 0.3914629337539432\n",
      "Epoch 3: train loss 0.6624000412942922, train accuracy 0.37379246894920154, validation loss 0.6719619302719549, validation accuracy 0.3914629337539432\n",
      "Epoch 4: train loss 0.6626508851523562, train accuracy 0.37392390090030886, validation loss 0.6707084682837646, validation accuracy 0.3914629337539432\n",
      "Epoch 5: train loss 0.6622872826084323, train accuracy 0.37379246894920154, validation loss 0.6714872840075087, validation accuracy 0.3914629337539432\n",
      "Epoch 6: train loss 0.6616638499709493, train accuracy 0.37359532102254056, validation loss 0.6714552298705284, validation accuracy 0.3914629337539432\n",
      "Epoch 7: train loss 0.6620057241326133, train accuracy 0.37379246894920154, validation loss 0.6715250368750058, validation accuracy 0.3914629337539432\n",
      "Epoch 8: train loss 0.6614454264019419, train accuracy 0.3736610369980942, validation loss 0.6712193188231074, validation accuracy 0.3914629337539432\n",
      "Epoch 9: train loss 0.6619437715889254, train accuracy 0.3737267529736479, validation loss 0.6705043449386813, validation accuracy 0.3914629337539432\n",
      "Epoch 10: train loss 0.6616264662135013, train accuracy 0.3738581849247552, validation loss 0.6706729380866331, validation accuracy 0.3914629337539432\n",
      "Epoch 11: train loss 0.661361804888707, train accuracy 0.37379246894920154, validation loss 0.6702224206097118, validation accuracy 0.3914629337539432\n",
      "Epoch 12: train loss 0.6615384084256946, train accuracy 0.3739896168758625, validation loss 0.6710022358864264, validation accuracy 0.3914629337539432\n",
      "Finished Training\n",
      "Training complete in 78m 36s\n"
     ]
    }
   ],
   "source": [
    "train_rezult_metrics = train_model(\n",
    "    num_of_epoch,\n",
    "    hwi_conv_neural_net,\n",
    "    dataset_loaders,\n",
    "    dataset_sizes,\n",
    "    criterion,\n",
    "    optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, validation_losses, train_accuracies, validation_accuracies = train_rezult_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(train_losses)), train_losses, label='train')\n",
    "plt.plot(np.arange(len(validation_losses)), validation_losses, label='validation')\n",
    "plt.legend()\n",
    "plt.title(\"loss by epoch\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(len(train_accuracies)), train_accuracies, label='train')\n",
    "plt.plot(np.arange(len(validation_accuracies)), validation_accuracies, label='validation')\n",
    "plt.legend()\n",
    "plt.title(\"accuracy by epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.770180433410817,\n",
       "  0.6627301558751334,\n",
       "  0.6624000412942922,\n",
       "  0.6626508851523562,\n",
       "  0.6622872826084323,\n",
       "  0.6616638499709493,\n",
       "  0.6620057241326133,\n",
       "  0.6614454264019419,\n",
       "  0.6619437715889254,\n",
       "  0.6616264662135013,\n",
       "  0.661361804888707,\n",
       "  0.6615384084256946],\n",
       " [0.6710197983856081,\n",
       "  0.6718500892070566,\n",
       "  0.6719619302719549,\n",
       "  0.6707084682837646,\n",
       "  0.6714872840075087,\n",
       "  0.6714552298705284,\n",
       "  0.6715250368750058,\n",
       "  0.6712193188231074,\n",
       "  0.6705043449386813,\n",
       "  0.6706729380866331,\n",
       "  0.6702224206097118,\n",
       "  0.6710022358864264],\n",
       " [0.37346388907143324,\n",
       "  0.37392390090030886,\n",
       "  0.37379246894920154,\n",
       "  0.37392390090030886,\n",
       "  0.37379246894920154,\n",
       "  0.37359532102254056,\n",
       "  0.37379246894920154,\n",
       "  0.3736610369980942,\n",
       "  0.3737267529736479,\n",
       "  0.3738581849247552,\n",
       "  0.37379246894920154,\n",
       "  0.3739896168758625],\n",
       " [0.3914629337539432,\n",
       "  0.3914629337539432,\n",
       "  0.3914629337539432,\n",
       "  0.3914629337539432,\n",
       "  0.3914629337539432,\n",
       "  0.3914629337539432,\n",
       "  0.3914629337539432,\n",
       "  0.3914629337539432,\n",
       "  0.3914629337539432,\n",
       "  0.3914629337539432,\n",
       "  0.3914629337539432,\n",
       "  0.3914629337539432])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rezult_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
